# -*- coding: utf-8 -*-
# module: future_jobs_consensus.py

from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple
import re
import json
from collections import defaultdict, Counter

# ---------- Domain vocab and heuristics (editable) ----------

JOB_SYNONYMS = {
    "معلم": ["آموزگار", "مدرس", "Teacher", "Tutor", "آموزشیار"],
    "پرستار": ["پرستار حرفه‌ای", "Registered Nurse", "RN", "پرستار بالینی"],
    "پزشک": ["Doctor", "General Practitioner", "GP", "متخصص"],
    "روانشناس": ["روان‌درمانگر", "Therapist", "Psychologist", "مشاور"],
    "طراح تجربه کاربری": ["UX Designer", "طراح UX/UI", "طراح محصول"],
    "مدیر محصول": ["Product Manager", "PM", "مدیر توسعه محصول"],
    "مدیر پروژه فناوری": ["IT Project Manager", "مدیر پروژه IT"],
    "معمار راهکار": ["Solution Architect", "Architect IT"],
    "تحلیلگر امنیت": ["Cybersecurity Analyst", "تحلیلگر سایبری", "امنیت اطلاعات"],
    "پژوهشگر علوم داده": ["Data Scientist", "پژوهشگر داده"],
    "متخصص رشد کسب‌وکار": ["Business Development", "Growth", "BD"],
    "طراح آموزشی": ["Instructional Designer", "Learning Designer"],
    "فیزیوتراپیست": ["Physiotherapist", "Physical Therapist"],
    "مددکار اجتماعی": ["Social Worker"],
    "سرآشپز": ["Chef", "آشپز حرفه‌ای"],
    "هنرمند": ["Artist", "موسیقیدان", "نقاش", "اجراگر"],
    "کارآفرین": ["Entrepreneur", "بنیان‌گذار"],
    "تحلیلگر سیاست عمومی": ["Policy Analyst", "تحلیلگر اندیشکده"],
    "مدیر جامعه و ارتباطات": ["Community Manager", "روابط عمومی نو"]
}

# Signals that a job is AI-resilient (editable, weighted)
RESILIENCE_SIGNALS = {
    "human_interaction": ["همدلی", "تعامل انسانی", "ارتباط چهره‌به‌چهره", "مذاکره", "مصاحبه", "حمایت عاطفی"],
    "creativity": ["خلاقیت", "طراحی", "ایده‌پردازی", "روایت", "بداهه"],
    "complex_judgment": ["قضاوت اخلاقی", "تصمیم‌گیری پیچیده", "مسئولیت‌پذیری بالینی", "خطر انسانی"],
    "physical_presence": ["مهارت دستی", "حضور فیزیکی", "مراقبت", "اجرای زنده"],
    "adaptation": ["یادگیری مداوم", "سازگاری", "میان‌رشته‌ای", "چندمهارتی"]
}
RESILIENCE_WEIGHTS = {
    "human_interaction": 2.2,
    "creativity": 2.0,
    "complex_judgment": 2.4,
    "physical_presence": 1.8,
    "adaptation": 1.6
}

# Skills dictionary (seed; extend per domain)
SKILL_MAP = {
    "نرم": ["ارتباط مؤثر", "همدلی", "تفکر انتقادی", "حل مسئله", "مدیریت زمان", "مذاکره", "تسهیل‌گری"],
    "فنی عمومی": ["علوم داده مقدماتی", "تحلیل آماری", "SQL", "مصرف مسئولانه AI", "اتوماسیون بدون‌کد", "امنیت پایه"],
    "دامنه‌ای": ["سواد سلامت", "سواد آموزشی", "سواد طراحی تجربه", "سیاست‌گذاری عمومی", "اخلاق حرفه‌ای"]
}

CREDENTIAL_HINTS = {
    "پرستار": ["RN/State License", "BLS/ACLS", "Clinical Rotations"],
    "پزشک": ["MD/DO", "Board Certification", "Residency"],
    "روانشناس": ["MSc/PhD Psychology", "License", "CBT/ACT"],
    "طراح تجربه کاربری": ["UX Nanodegree", "HCI Certificate", "Design Portfolio"],
    "مدیر محصول": ["PM Certificate", "Scrum/PSPO", "Discovery/Outcome"],
    "تحلیلگر امنیت": ["Security+", "CEH", "CISSP (mid/long term)"],
    "پژوهشگر علوم داده": ["Python/ML Cert", "Statistics", "MLOps (اختیاری)"],
    "فیزیوتراپیست": ["DPT/License", "Manual Therapy"],
    "مددکار اجتماعی": ["MSW/License"],
    "سرآشپز": ["Culinary Diploma", "Food Safety"],
    "تحلیلگر سیاست عمومی": ["MPP/MA", "Policy Evaluation"]
}

# ---------- Data models ----------

@dataclass
class Article:
    id: str
    title: str
    text: str
    source: Optional[str] = None
    year: Optional[int] = None
    credibility: float = 1.0  # allow manual override 0.5–2.0

@dataclass
class JobInsight:
    job_title: str
    mentions: int
    consensus_score: float
    ai_risk_score: float
    demand_outlook: str
    regions: List[str]
    skills_core: List[str]
    skills_plus: List[str]
    credentials: List[str]
    roadmap: List[Dict[str, Any]]
    sources: List[str]

# ---------- Utilities ----------

def normalize_text(t: str) -> str:
    return re.sub(r"\s+", " ", t).strip().lower()

def detect_job_mentions(text: str) -> Dict[str, int]:
    counts = Counter()
    for canon, aliases in JOB_SYNONYMS.items():
        terms = [canon] + aliases
        for term in terms:
            pattern = r"\b" + re.escape(term.lower()) + r"\b"
            matches = re.findall(pattern, text.lower())
            if matches:
                counts[canon] += len(matches)
    return counts

def score_resilience(text: str) -> float:
    score = 0.0
    for k, keywords in RESILIENCE_SIGNALS.items():
        w = RESILIENCE_WEIGHTS.get(k, 1.0)
        hits = sum(text.count(kw.lower()) for kw in [x.lower() for x in keywords])
        if hits:
            score += w * min(hits, 3)  # cap contribution per signal
    # scale to 0-10
    return min(10.0, round(score, 2))

def infer_demand_outlook(text: str) -> str:
    # simple heuristic — can be replaced with classifier
    if any(k in text for k in ["رشد", "افزایش تقاضا", "کمبود نیرو", "محرک تقاضا"]):
        return "High"
    if any(k in text for k in ["ثابت", "میانه", "متوسط"]):
        return "Medium"
    if any(k in text for k in ["کاهش", "افول", "اشباع"]):
        return "Low"
    return "Medium"

def guess_regions(text: str) -> List[str]:
    regions = []
    if any(k in text for k in ["اروپا", "آلمان", "اتحادیه اروپا", "آسیای مرکزی"]):
        regions.append("EU/DE")
    if any(k in text for k in ["آمریکا", "ایالات متحده", "us", "usa"]):
        regions.append("US")
    if any(k in text for k in ["ایران", "خاورمیانه", "mena"]):
        regions.append("MENA/IR")
    return regions or ["Global"]

def suggest_skills(job: str) -> Tuple[List[str], List[str]]:
    core, plus = [], []
    # generic mapping
    if job in ["روانشناس", "پرستار", "پزشک", "مددکار اجتماعی", "فیزیوتراپیست"]:
        core = ["همدلی", "ارتباط مؤثر", "اخلاق حرفه‌ای", "مستندسازی بالینی"]
        plus = ["تحلیل داده سلامت", "مصرف مسئولانه AI", "آموزش بیمار", "کیفیت و ایمنی"]
    elif job in ["طراح تجربه کاربری", "هنرمند", "سرآشپز"]:
        core = ["خلاقیت", "تحقیق کاربر/مخاطب", "نمونه‌سازی سریع", "روایت‌پردازی"]
        plus = ["تحلیل رفتار/داده", "اتوماسیون بدون‌کد", "A/B تست", "دسترس‌پذیری"]
    elif job in ["مدیر محصول", "مدیر پروژه فناوری", "معمار راهکار", "متخصص رشد کسب‌وکار"]:
        core = ["تفکر سیستم", "اولویت‌بندی", "همسوسازی ذی‌نفعان", "مدیریت ریسک"]
        plus = ["SQL برای PM", "لو-کد/نو-کد", "تحلیل آزمایشی", "قابلیت‌سنجی فنی"]
    elif job in ["تحلیلگر امنیت", "پژوهشگر علوم داده"]:
        core = ["تحلیل مسئله", "اخلاق داده", "مستندسازی", "تفکر آماری"]
        plus = ["MLOps/DevSecOps (انتخابی)", "اتوماسیون", "Visual Analytics"]
    else:
        core = ["ارتباط مؤثر", "حل مسئله", "سازگاری"]
        plus = ["مصرف مسئولانه AI", "تحلیل داده کاربردی"]
    # add seeds
    plus = list(dict.fromkeys(plus + SKILL_MAP["نرم"][:2] + ["مدیریت زمان"]))
    return core, plus

def suggest_credentials(job: str) -> List[str]:
    return CREDENTIAL_HINTS.get(job, ["Portfolio/Case Studies", "Certificate از پلتفرم معتبر"])

def build_roadmap(job: str) -> List[Dict[str, Any]]:
    steps = []
    if job in ["طراح تجربه کاربری", "مدیر محصول", "پژوهشگر علوم داده", "تحلیلگر امنیت"]:
        steps = [
            {"step_no": 1, "goal": "پایه‌ها و سواد حوزه", "resources": ["کتاب مرجع", "دوره مقدماتی"], "practice_tasks": ["یادداشت‌برداری ساختاری"], "deliverable": "خلاصه مفهومی", "timebox_weeks": 2},
            {"step_no": 2, "goal": "مهارت‌های عملی کلیدی", "resources": ["دوره‌های hands-on"], "practice_tasks": ["تمرین‌های هفتگی"], "deliverable": "ریپوی تمرین", "timebox_weeks": 4},
            {"step_no": 3, "goal": "پروژه کاربردی با داده/کاربر واقعی", "resources": ["Datasets/مصاحبه کاربر"], "practice_tasks": ["پیاده‌سازی انتهابه‌انتها"], "deliverable": "پروژه مستند", "timebox_weeks": 4},
            {"step_no": 4, "goal": "استانداردها و گواهی", "resources": ["نمونه سوال/راهنما"], "practice_tasks": ["ماک‌تست"], "deliverable": "گواهی/Badge", "timebox_weeks": 2},
            {"step_no": 5, "goal": "برندسازی و شبکه‌سازی", "resources": ["لينکدین/انجمن‌ها"], "practice_tasks": ["پست تحلیلی", "Mentorship"], "deliverable": "پورتفولیو", "timebox_weeks": 2}
        ]
    else:
        steps = [
            {"step_no": 1, "goal": "پایه‌ها و سواد حوزه", "resources": [], "practice_tasks": [], "deliverable": "خلاصه مفاهیم", "timebox_weeks": 2},
            {"step_no": 2, "goal": "تمرین عملی هدایت‌شده", "resources": [], "practice_tasks": [], "deliverable": "نمونه کار", "timebox_weeks": 3},
            {"step_no": 3, "goal": "استانداردها/مجوز", "resources": [], "practice_tasks": [], "deliverable": "مدرک/مجوز", "timebox_weeks": 2}
        ]
    return steps

# ---------- Core pipeline ----------

def analyze_articles(articles: List[Article]) -> Dict[str, Any]:
    # aggregate signals across 30–40 articles
    job_counter = Counter()
    job_sources = defaultdict(list)
    job_texts = defaultdict(list)
    credibility_map = {}

    for a in articles:
        t = normalize_text(a.text)
        credibility_map[a.id] = a.credibility
        counts = detect_job_mentions(t)
        for job, c in counts.items():
            job_counter[job] += int(c * a.credibility)
            job_sources[job].append(a.source or a.title or a.id)
            job_texts[job].append(t)

    # build insights
    insights: List[JobInsight] = []
    for job, mentions in job_counter.items():
        combined_text = " ".join(job_texts[job])
        ai_resilience = score_resilience(combined_text)
        outlook = infer_demand_outlook(combined_text)
        regions = list(dict.fromkeys(sum([guess_regions(txt) for txt in job_texts[job]], [])))
        core, plus = suggest_skills(job)
        creds = suggest_credentials(job)
        roadmap = build_roadmap(job)

        # consensus_score considers mentions volume and diversity of sources
        source_diversity = len(set(job_sources[job]))
        consensus = round(min(10.0, 0.5 * (mentions ** 0.5) + 0.8 * (source_diversity ** 0.5)), 2)

        insights.append(
            JobInsight(
                job_title=job,
                mentions=mentions,
                consensus_score=consensus,
                ai_risk_score=round(10 - ai_resilience, 2),  # lower risk = higher resilience
                demand_outlook=outlook,
                regions=regions,
                skills_core=core,
                skills_plus=plus,
                credentials=creds,
                roadmap=roadmap,
                sources=list(dict.fromkeys(job_sources[job]))[:10]
            )
        )

    # rank by combined priority: high consensus, low AI risk, high outlook
    def rank_key(j: JobInsight):
        outlook_weight = {"High": 1.0, "Medium": 0.6, "Low": 0.2}.get(j.demand_outlook, 0.6)
        return (j.consensus_score * 1.1) + ((10 - j.ai_risk_score) * 1.2) + (outlook_weight * 2.0)

    insights_sorted = sorted(insights, key=rank_key, reverse=True)

    # build final JSON
    result = {
        "summary": {
            "articles_processed": len(articles),
            "jobs_detected": len(insights_sorted),
            "top_jobs": [i.job_title for i in insights_sorted[:10]]
        },
        "jobs": [asdict(i) for i in insights_sorted]
    }
    return result

# ---------- Public API ----------

def run_pipeline(raw_articles: List[Dict[str, Any]]) -> str:
    """
    raw_articles: list of dicts like
      { "id": "...", "title": "...", "text": "...", "source": "...", "year": 2024, "credibility": 1.2 }
    returns JSON string
    """
    arts = [Article(**a) for a in raw_articles]
    out = analyze_articles(arts)
    return json.dumps(out, ensure_ascii=False, indent=2)

# ---------- Example stub ----------
if __name__ == "__main__":
    demo_articles = [
        {
            "id": "a1",
            "title": "تحلیل مشاغل آینده در اروپا",
            "text": "در این گزارش، نقش‌های پرستار، روانشناس و طراح تجربه کاربری با رشد تقاضا معرفی شدند. "
                    "دلایل: تعامل انسانی، همدلی، قضاوت اخلاقی و نیاز به خلاقیت. بازار آلمان و اتحادیه اروپا محور گزارش بود.",
            "source": "ThinkTank EU 2025",
            "year": 2025,
            "credibility": 1.3
        },
        {
            "id": "a2",
            "title": "مقاومت مشاغل در برابر اتوماسیون",
            "text": "مدیر محصول، تحلیلگر امنیت، پزشک و مددکار اجتماعی به دلیل تصمیم‌گیری پیچیده، حضور فیزیکی و سازگاری مداوم "
                    "در برابر اتوماسیون مقاوم‌اند. کمبود نیرو گزارش شده و رشد ادامه دارد. آمریکا، اروپا و خاورمیانه.",
            "source": "Policy Lab 2024",
            "year": 2024,
            "credibility": 1.1
        }
    ]
    print(run_pipeline(demo_articles))
